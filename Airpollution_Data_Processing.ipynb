{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fdc39dda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Arizona', 'California', 'Colorado', 'District Of Columbia', 'Florida', 'Illinois', 'Indiana', 'Kansas', 'Kentucky', 'Louisiana', 'Michigan', 'Missouri', 'New Jersey', 'New York', 'North Carolina', 'Oklahoma', 'Pennsylvania', 'Texas', 'Virginia', 'Massachusetts', 'Nevada', 'New Hampshire', 'Tennessee', 'South Carolina', 'Connecticut', 'Iowa', 'Maine', 'Maryland', 'Wisconsin', 'Country Of Mexico', 'Arkansas', 'Oregon', 'Wyoming', 'North Dakota', 'Idaho', 'Ohio', 'Georgia', 'Delaware', 'Hawaii', 'Minnesota', 'New Mexico', 'Rhode Island', 'South Dakota', 'Utah', 'Alabama', 'Washington', 'Alaska'] 47\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import tree\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import time\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "data = \"s3://pollution-predictor-1/uspollution_pollution_us_2000_2016.csv\"\n",
    "df = pd.read_csv(data)\n",
    "column_names=[\"State Code\",\"County Code\",\"Site Num\",\"Address\",\"State\",\"County\",\"City\",\"Date Local\",\n",
    "             \"NO2 Mean\",\"NO2 1st Max Value\",\"NO2 1st Max Hour\",\"NO2 AQI\",\"O3 Mean\",\"O3 1st Max Value\",\n",
    "              \"O3 1st Max Hour\",\"O3 AQI\",\"SO2 Mean\",\"SO2 1st Max Value\",\"SO2 1st Max Hour\",\"SO2 AQI\",\n",
    "              \"CO Mean\",\"CO 1st Max Value\",\"CO 1st Max Hour\",\"CO AQI\"]\n",
    "\n",
    "del df['Unnamed: 0']\n",
    "del df['NO2 Units']\n",
    "del df['O3 Units']\n",
    "del df['SO2 Units']\n",
    "del df['CO Units']\n",
    "\n",
    "states=[]\n",
    "for i in df[\"State\"]:\n",
    "    if i not in states:\n",
    "        states.append(i)\n",
    "print(states,len(states))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4f95ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "\n",
    "df['State'] = le.fit_transform(df['State'])\n",
    "state_map = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "\n",
    "df['County'] = le.fit_transform(df['County'])\n",
    "county_map = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "\n",
    "df['City'] = le.fit_transform(df['City'])\n",
    "city_map = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "\n",
    "df['Address'] = le.fit_transform(df['Address'])\n",
    "address_map = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "\n",
    "df['Date Local'] = le.fit_transform(df['Date Local'])\n",
    "datelocal_map = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "\n",
    "def replace_with_nan(df_col, labels):\n",
    "    for key in labels.keys():\n",
    "        if type(key) == float:\n",
    "            df_col = df_col.replace(labels[key], np.NaN)\n",
    "    return df_col\n",
    "\n",
    "df['State'] = replace_with_nan(df['State'], state_map)\n",
    "df['County'] = replace_with_nan(df['County'], county_map)\n",
    "df['City'] = replace_with_nan(df['City'], city_map)\n",
    "df['Address'] = replace_with_nan(df['Address'], address_map)\n",
    "df['Date Local'] = replace_with_nan(df['Date Local'], datelocal_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ead6158",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "df_copy = df.copy(deep = True)\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "imputed = imputer.fit_transform(df)\n",
    "\n",
    "df_imputed = pd.DataFrame(imputed, columns = column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5e7047",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "scaled_array = scaler.fit_transform(df_imputed.values)\n",
    "\n",
    "df_scaled = pd.DataFrame(scaled_array, columns = column_names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
